<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>arrow2 documentation</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Arrow2</a></li><li class="chapter-item expanded "><a href="arrow.html"><strong aria-hidden="true">2.</strong> The arrow format</a></li><li class="chapter-item expanded "><a href="low_level.html"><strong aria-hidden="true">3.</strong> Low-level API</a></li><li class="chapter-item expanded "><a href="high_level.html"><strong aria-hidden="true">4.</strong> High-level API</a></li><li class="chapter-item expanded "><a href="compute.html"><strong aria-hidden="true">5.</strong> Compute</a></li><li class="chapter-item expanded "><a href="metadata.html"><strong aria-hidden="true">6.</strong> Metadata</a></li><li class="chapter-item expanded "><a href="ffi.html"><strong aria-hidden="true">7.</strong> Foreign interfaces</a></li><li class="chapter-item expanded "><a href="extension.html"><strong aria-hidden="true">8.</strong> Extension</a></li><li class="chapter-item expanded "><a href="io/index.html"><strong aria-hidden="true">9.</strong> IO</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="io/csv_reader.html"><strong aria-hidden="true">9.1.</strong> Read CSV</a></li><li class="chapter-item expanded "><a href="io/csv_write.html"><strong aria-hidden="true">9.2.</strong> Write CSV</a></li><li class="chapter-item expanded "><a href="io/parquet_read.html"><strong aria-hidden="true">9.3.</strong> Read Parquet</a></li><li class="chapter-item expanded "><a href="io/parquet_write.html"><strong aria-hidden="true">9.4.</strong> Write Parquet</a></li><li class="chapter-item expanded "><a href="io/ipc_read.html"><strong aria-hidden="true">9.5.</strong> Read Arrow</a></li><li class="chapter-item expanded "><a href="io/ipc_stream_read.html"><strong aria-hidden="true">9.6.</strong> Read Arrow stream</a></li><li class="chapter-item expanded "><a href="io/ipc_write.html"><strong aria-hidden="true">9.7.</strong> Write Arrow</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">arrow2 documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="arrow2"><a class="header" href="#arrow2">Arrow2</a></h1>
<p>Arrow2 is a Rust library that implements data structures and functionality enabling
interoperability with the arrow format.</p>
<p>The typical use-case for this library is to perform CPU and memory-intensive analytics in a format that supports heterogeneous data structures, null values, and IPC and FFI interfaces across languages.</p>
<p>Arrow2 is divided into three main parts: </p>
<ul>
<li>a <a href="./low_level.html">low-level API</a> to efficiently operate with contiguous memory regions;</li>
<li>a <a href="./high_level.html">high-level API</a> to operate with arrow arrays;</li>
<li>a <a href="./metadata.html">metadata API</a> to declare and operate with logical types and metadata.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to the Arrow2 guide for the Rust programming language. This guide was
created to help you become familiar with the Arrow2 crate and its
functionalities.</p>
<h2 id="what-is-apache-arrow"><a class="header" href="#what-is-apache-arrow">What is Apache Arrow?</a></h2>
<p>According to its <a href="https://arrow.apache.org">website</a> Apache Arrow is defined
as:</p>
<blockquote>
<p>A language-independent columnar memory format for flat and hierarchical data,
organized for efficient analytic operations on modern hardware like CPUs and
GPUs. The Arrow memory format also supports zero-copy reads for
lightning-fast data access without serialization overhead.</p>
</blockquote>
<p>After reading the description you have probably come to the conclusion that
Apache Arrow sounds great and that it will give anyone working with data enough
tools to improve a data processing workflow.  But that's the catch, on its own
Apache Arrow is not an application or library that can be installed and used.
The objective of Apache Arrow is to define a set of specifications that need to
be followed by an implementation in order to allow:</p>
<ol>
<li>fast in-memory data access</li>
<li>sharing and zero copy of data between processes</li>
</ol>
<h3 id="fast-in-memory-data-access"><a class="header" href="#fast-in-memory-data-access">Fast in-memory data access</a></h3>
<p>Apache Arrow allows fast memory access by defining its <a href="https://arrow.apache.org/overview/">in-memory columnar
format</a>. This columnar format defines a
standard and efficient in-memory representation of various datatypes, plain or
nested
(<a href="https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst">reference</a>).</p>
<p>In other words, the Apache Arrow project has created a series of rules or
specifications to define how a datatype (int, float, string, list, etc.) is
stored in memory. Since the objective of the project is to store large amounts
of data in memory for further manipulation or querying, it uses a columnar data
definition. This means that when a dataset (data defined with several columns)
is stored in memory, it no longer maintains its rows representation but it is
changed to a columnar representation.</p>
<p>For example, lets say we have a dataset that is defined with three columns
named: <em>session_id</em>, <em>timestamp</em> and <em>source_id</em> (image below). Traditionally,
this file should be represented in memory maintaining its row representation
(image below, left). This means that the fields representing a row would be kept
next to each other. This makes memory management harder to achieve because there
are different datatypes next to each other; in this case a long, a date and a
string. Each of these datatypes will have different memory requirements (for
example, 8 bytes, 16 bytes or 32 bytes).</p>
<p align="center">
  <img src="images/simd.png">
</p>
<p>By changing the in memory representation of the file to a columnar form (image
above, right), the in-memory arrangement of the data becomes more efficient.
Similar datatypes are stored next to each other, making the access and columnar
querying faster to perform.</p>
<h3 id="sharing-data-between-processes"><a class="header" href="#sharing-data-between-processes">Sharing data between processes</a></h3>
<p>Imagine a typical workflow for a data engineer. There is a process that is
producing data that belongs to a service monitoring the performance of a sales
page.  This data has to be read, processed and stored. Probably the engineer
would first set a script that is reading the data and storing the result in a
CSV or Parquet file. Then the engineer would need to create a pipeline to read
the file and transfer the data to a database. Once the data is stored some
analysis is needed to be done on the data, maybe Pandas is used to read the data
and extract information. Or, perhaps Spark is used to create a pipeline that
reads the database in order to create a stream of data to feed a dashboard. The
copy and convert process may end up looking like this:</p>
<p align="center">
  <img src="images/copy.png">
</p>
<p>As it can be seen, the data is copied and converted several times. This happens
every time a process needs to query the data.</p>
<p>By using a standard that all languages and processes can understand, the data
doesn't need to be copied and converted. There can be a single in-memory data
representation that can be used to feed all the required processes. The data
sharing can be done regarding the language that is used.</p>
<p align="center">
  <img src="images/shared.png">
</p>
<p>And thanks to this standardization the data can also be shared with processes
that don't share the same memory. By creating a data server, packets of data
with known structure (RecordBatch) can be sent across computers (or pods) and
the receiving process doesn't need to spend time coding and decoding the data
to a known format. The data is ready to be used once its being received.</p>
<p align="center">
  <img src="images/recordbatch.png">
</p>
<h2 id="the-arrow2-crate"><a class="header" href="#the-arrow2-crate">The Arrow2 crate</a></h2>
<p>These and other collateral benefits can only be achieved thanks to the work done
by the people collaborating in the Apache Arrow project. By looking at the
project <a href="https://github.com/apache/arrow">github page</a>, there are libraries for
the most common languages used today, and that includes Rust.</p>
<p>The Rust Arrow2 crate is a collection of structs and implementations that define
all the elements required to create Arrow arrays that follow the Apache Arrow
specification. In the next sections the basic blocks for working with the
crate will be discussed, providing enough examples to give you familiarity
to construct, share and query Arrow arrays.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="low-level-api"><a class="header" href="#low-level-api">Low-level API</a></h1>
<p>The starting point of this crate is the idea that data is stored in memory in a specific arrangement to be interoperable with Arrow's ecosystem.</p>
<p>The most important design aspect of this crate is that contiguous regions are shared via an
<code>Arc</code>. In this context, the operation of slicing a memory region is <code>O(1)</code> because it
corresponds to changing an offset and length. The tradeoff is that once under
an <code>Arc</code>, memory regions are immutable.</p>
<p>The second most important aspect is that Arrow has two main types of data buffers: bitmaps,
whose offsets are measured in bits, and byte types (such as <code>i32</code>), whose offsets are
measured in bytes. With this in mind, this crate has 2 main types of containers of
contiguous memory regions:</p>
<ul>
<li><code>Buffer&lt;T&gt;</code>: handle contiguous memory regions of type T whose offsets are measured in items</li>
<li><code>Bitmap</code>: handle contiguous memory regions of bits whose offsets are measured in bits</li>
</ul>
<p>These hold <em>all</em> data-related memory in this crate.</p>
<p>Due to their intrinsic immutability, each container has a corresponding mutable
(and non-shareable) variant:</p>
<ul>
<li><code>MutableBuffer&lt;T&gt;</code></li>
<li><code>MutableBitmap</code></li>
</ul>
<p>Let's see how these structures are used.</p>
<p>Create a new <code>Buffer&lt;u32&gt;</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::buffer::Buffer;
</span><span class="boring">fn main() {
</span>let x = Buffer::from(&amp;[1u32, 2, 3]);
assert_eq!(x.as_slice(), &amp;[1u32, 2, 3]);

let x = x.slice(1, 2);
assert_eq!(x.as_slice(), &amp;[2, 3]);
<span class="boring">}
</span></code></pre></pre>
<p>Using a <code>MutableBuffer&lt;i64&gt;</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::buffer::MutableBuffer;
</span><span class="boring">fn main() {
</span>let mut x: MutableBuffer&lt;i64&gt; = (0..3).collect();
x[1] = 5;
x.push(10);
assert_eq!(x.as_slice(), &amp;[0, 5, 2, 10])
<span class="boring">}
</span></code></pre></pre>
<p>The following demonstrates how to efficiently
perform an operation from an iterator of
<a href="https://doc.rust-lang.org/std/iter/trait.TrustedLen.html">TrustedLen</a>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::buffer::MutableBuffer;
</span><span class="boring">fn main() {
</span>let x = (0..1000).collect::&lt;Vec&lt;_&gt;&gt;();
let y = MutableBuffer::from_trusted_len_iter(x.iter().map(|x| x * 2));
assert_eq!(y[50], 100);
<span class="boring">}
</span></code></pre></pre>
<p>Using <code>from_trusted_len_iter</code> often causes the compiler to auto-vectorize.</p>
<p>In this context, <code>MutableBuffer</code> has an almost identical API to Rust's <code>Vec</code>.
However, contrarily to <code>Vec</code>, <code>Buffer</code> and <code>MutableBuffer</code> only supports
the following physical types:</p>
<ul>
<li><code>i8-i128</code></li>
<li><code>u8-u64</code></li>
<li><code>f32</code> and <code>f64</code></li>
<li><code>arrow2::types::days_ms</code></li>
<li><code>arrow2::types::months_days_ns</code></li>
</ul>
<p>This is because the arrow specification only supports the above Rust types; all other complex
types supported by arrow are built on top of these types, which enables Arrow to be a highly
interoperable in-memory format.</p>
<h2 id="bitmaps"><a class="header" href="#bitmaps">Bitmaps</a></h2>
<p>Arrow's in-memory arrangement of boolean values is different from <code>Vec&lt;bool&gt;</code>. Specifically,
arrow uses individual bits to represent a boolean, as opposed to the usual byte that <code>bool</code> holds.
Besides the 8x compression, this makes the validity particularly useful for 
<a href="https://en.wikipedia.org/wiki/AVX-512">AVX512</a> masks.
One tradeoff is that an arrows' bitmap is not represented as a Rust slice, as Rust slices use
pointer arithmetics, whose smallest unit is a byte.</p>
<p>Arrow2 has two containers for bitmaps: <code>Bitmap</code> (immutable and sharable)
and <code>MutableBitmap</code> (mutable):</p>
<pre><pre class="playground"><code class="language-rust">use arrow2::bitmap::Bitmap;
<span class="boring">fn main() {
</span>let x = Bitmap::from(&amp;[true, false]);
let iter = x.iter().map(|x| !x);
let y = Bitmap::from_trusted_len_iter(iter);
assert_eq!(y.get_bit(0), false);
assert_eq!(y.get_bit(1), true);
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust">use arrow2::bitmap::MutableBitmap;
<span class="boring">fn main() {
</span>let mut x = MutableBitmap::new();
x.push(true);
x.push(false);
assert_eq!(x.get(1), false);
x.set(1, true);
assert_eq!(x.get(1), true);
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="high-level-api"><a class="header" href="#high-level-api">High-level API</a></h1>
<p>Arrow core trait the <code>Array</code>, which you can think of as representing <code>Arc&lt;Vec&lt;Option&lt;T&gt;&gt;</code>
with associated metadata (see <a href="../metadata.html">metadata</a>)).
Contrarily to <code>Arc&lt;Vec&lt;Option&lt;T&gt;&gt;</code>, arrays in this crate are represented in such a way
that they can be zero-copied to any other Arrow implementation via foreign interfaces (FFI).</p>
<p>Probably the simplest <code>Array</code> in this crate is the <code>PrimitiveArray&lt;T&gt;</code>. It can be
constructed as from a slice of option values,</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::array::{Array, PrimitiveArray};
</span><span class="boring">fn main() {
</span>let array = PrimitiveArray::&lt;i32&gt;::from([Some(1), None, Some(123)]);
assert_eq!(array.len(), 3)
<span class="boring">}
</span></code></pre></pre>
<p>from a slice of values,</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::array::{Array, PrimitiveArray};
</span><span class="boring">fn main() {
</span>let array = PrimitiveArray::&lt;f32&gt;::from_slice([1.0, 0.0, 123.0]);
assert_eq!(array.len(), 3)
<span class="boring">}
</span></code></pre></pre>
<p>or from an iterator</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::array::{Array, PrimitiveArray};
</span><span class="boring">fn main() {
</span>let array: PrimitiveArray&lt;u64&gt; = [Some(1), None, Some(123)].iter().collect();
assert_eq!(array.len(), 3)
<span class="boring">}
</span></code></pre></pre>
<p>A <code>PrimitiveArray</code> has 3 components:</p>
<ol>
<li>A physical type (e.g. <code>i32</code>)</li>
<li>A logical type (e.g. <code>DataType::Int32</code>)</li>
<li>Data</li>
</ol>
<p>The main differences from a <code>Vec&lt;Option&lt;T&gt;&gt;</code> are:</p>
<ul>
<li>Its data is laid out in memory as a <code>Buffer&lt;T&gt;</code> and an <code>Option&lt;Bitmap&gt;</code> (see [../low_level.md])</li>
<li>It has an associated logical type (<code>DataType</code>).</li>
</ul>
<p>The first allows interoperability with Arrow's ecosystem and efficient SIMD operations
(we will re-visit this below); the second is that it gives semantic meaning to the array.
In the example</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::array::PrimitiveArray;
</span><span class="boring">use arrow2::datatypes::DataType;
</span><span class="boring">fn main() {
</span>let ints = PrimitiveArray::&lt;i32&gt;::from([Some(1), None]);
let dates = PrimitiveArray::&lt;i32&gt;::from([Some(1), None]).to(DataType::Date32);
<span class="boring">}
</span></code></pre></pre>
<p><code>ints</code> and <code>dates</code> have the same in-memory representation but different logic
representations (e.g. dates are usually printed to users as &quot;yyyy-mm-dd&quot;).</p>
<p>All physical types (e.g. <code>i32</code>) have a &quot;natural&quot; logical <code>DataType</code> (e.g. <code>DataType::Int32</code>)
which is assigned when allocating arrays from iterators, slices, etc.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::array::{Array, Int32Array, PrimitiveArray};
</span><span class="boring">use arrow2::datatypes::DataType;
</span><span class="boring">fn main() {
</span>let array = PrimitiveArray::&lt;i32&gt;::from_slice([1, 0, 123]);
assert_eq!(array.data_type(), &amp;DataType::Int32);
<span class="boring">}
</span></code></pre></pre>
<p>they can be cheaply (<code>O(1)</code>) converted to via <code>.to(DataType)</code>.</p>
<p>The following arrays are supported:</p>
<ul>
<li><code>NullArray</code> (just holds nulls)</li>
<li><code>BooleanArray</code> (booleans)</li>
<li><code>PrimitiveArray&lt;T&gt;</code> (for ints, floats)</li>
<li><code>Utf8Array&lt;i32&gt;</code> and <code>Utf8Array&lt;i64&gt;</code> (for strings)</li>
<li><code>BinaryArray&lt;i32&gt;</code> and <code>BinaryArray&lt;i64&gt;</code> (for opaque binaries)</li>
<li><code>FixedSizeBinaryArray</code> (like <code>BinaryArray</code>, but fixed size)</li>
<li><code>ListArray&lt;i32&gt;</code> and <code>ListArray&lt;i64&gt;</code> (nested arrays)</li>
<li><code>FixedSizeListArray</code> (nested arrays of fixed size)</li>
<li><code>StructArray</code> (every row has multiple logical types)</li>
<li><code>UnionArray</code> (every row has a different logical type)</li>
<li><code>DictionaryArray&lt;K&gt;</code> (nested array with encoded values)</li>
</ul>
<h2 id="array-as-a-trait-object"><a class="header" href="#array-as-a-trait-object">Array as a trait object</a></h2>
<p><code>Array</code> is object safe, and all implementations of <code>Array</code> and can be casted
to <code>&amp;dyn Array</code>, which enables run-time nesting.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::array::{Array, PrimitiveArray};
</span><span class="boring">fn main() {
</span>let a = PrimitiveArray::&lt;i32&gt;::from(&amp;[Some(1), None]);
let a: &amp;dyn Array = &amp;a;
<span class="boring">}
</span></code></pre></pre>
<h3 id="downcast-and-as_any"><a class="header" href="#downcast-and-as_any">Downcast and <code>as_any</code></a></h3>
<p>Given a trait object <code>array: &amp;dyn Array</code>, we know its physical type via
<code>PhysicalType: array.data_type().to_physical_type()</code>, which we use to downcast the array
to its concrete physical type:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::array::{Array, PrimitiveArray};
</span><span class="boring">use arrow2::datatypes::PhysicalType;
</span><span class="boring">fn main() {
</span>let array = PrimitiveArray::&lt;i32&gt;::from(&amp;[Some(1), None]);
let array = &amp;array as &amp;dyn Array;
// ...
let physical_type: PhysicalType = array.data_type().to_physical_type();
<span class="boring">}
</span></code></pre></pre>
<p>There is a one to one relationship between each variant of <code>PhysicalType</code> (an enum) and
an each implementation of <code>Array</code> (a struct):</p>
<table><thead><tr><th><code>PhysicalType</code></th><th><code>Array</code></th></tr></thead><tbody>
<tr><td><code>Primitive(_)</code></td><td><code>PrimitiveArray&lt;_&gt;</code></td></tr>
<tr><td><code>Binary</code></td><td><code>BinaryArray&lt;i32&gt;</code></td></tr>
<tr><td><code>LargeBinary</code></td><td><code>BinaryArray&lt;i64&gt;</code></td></tr>
<tr><td><code>Utf8</code></td><td><code>Utf8Array&lt;i32&gt;</code></td></tr>
<tr><td><code>LargeUtf8</code></td><td><code>Utf8Array&lt;i64&gt;</code></td></tr>
<tr><td><code>List</code></td><td><code>ListArray&lt;i32&gt;</code></td></tr>
<tr><td><code>LargeList</code></td><td><code>ListArray&lt;i64&gt;</code></td></tr>
<tr><td><code>FixedSizeBinary</code></td><td><code>FixedSizeBinaryArray</code></td></tr>
<tr><td><code>FixedSizeList</code></td><td><code>FixedSizeListArray</code></td></tr>
<tr><td><code>Struct</code></td><td><code>StructArray</code></td></tr>
<tr><td><code>Union</code></td><td><code>UnionArray</code></td></tr>
<tr><td><code>Map</code></td><td><code>MapArray</code></td></tr>
<tr><td><code>Dictionary(_)</code></td><td><code>DictionaryArray&lt;_&gt;</code></td></tr>
</tbody></table>
<p>where <code>_</code> represents each of the variants (e.g. <code>PrimitiveType::Int32 &lt;-&gt; i32</code>).</p>
<p>In this context, a common idiom in using <code>Array</code> as a trait object is as follows:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use arrow2::datatypes::{PhysicalType, PrimitiveType};
use arrow2::array::{Array, PrimitiveArray};

fn float_operator(array: &amp;dyn Array) -&gt; Result&lt;Box&lt;dyn Array&gt;, String&gt; {
    match array.data_type().to_physical_type() {
        PhysicalType::Primitive(PrimitiveType::Float32) =&gt; {
            let array = array.as_any().downcast_ref::&lt;PrimitiveArray&lt;f32&gt;&gt;().unwrap();
            // let array = f32-specific operator
            let array = array.clone();
            Ok(Box::new(array))
        }
        PhysicalType::Primitive(PrimitiveType::Float64) =&gt; {
            let array = array.as_any().downcast_ref::&lt;PrimitiveArray&lt;f64&gt;&gt;().unwrap();
            // let array = f64-specific operator
            let array = array.clone();
            Ok(Box::new(array))
        }
        _ =&gt; Err(&quot;This operator is only valid for float point arrays&quot;.to_string()),
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="from-iterator"><a class="header" href="#from-iterator">From Iterator</a></h2>
<p>In the examples above, we've introduced how to create an array from an iterator.
These APIs are available for all Arrays, and they are suitable to efficiently
create them. In this section we will go a bit more in detail about these operations,
and how to make them even more efficient.</p>
<p>This crate's APIs are generally split into two patterns: whether an operation leverages
contiguous memory regions or whether it does not.</p>
<p>What this means is that certain operations can be performed irrespectively of whether a value
is &quot;null&quot; or not (e.g. <code>PrimitiveArray&lt;i32&gt; + i32</code> can be applied to <em>all</em> values via SIMD and 
only copy the validity bitmap independently).</p>
<p>When an operation benefits from such arrangement, it is advantageous to use</p>
<ul>
<li><code>Buffer::from_iter</code></li>
<li><code>Buffer::from_trusted_len_iter</code></li>
<li><code>Buffer::try_from_trusted_len_iter</code></li>
</ul>
<p>If not, then use the <code>MutableArray</code> API, such as
<code>MutablePrimitiveArray&lt;T&gt;</code>, <code>MutableUtf8Array&lt;O&gt;</code> or <code>MutableListArray</code>.</p>
<p>We have seen examples where the latter API was used. In the last example of this page
you will be introduced to an example of using the former for SIMD.</p>
<h2 id="into-iterator"><a class="header" href="#into-iterator">Into Iterator</a></h2>
<p>We've already seen how to create an array from an iterator. Most arrays also implement
<code>IntoIterator</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">use arrow2::array::{Array, Int32Array};
</span><span class="boring">fn main() {
</span>let array = Int32Array::from(&amp;[Some(1), None, Some(123)]);

for item in array.iter() {
    if let Some(value) = item {
        println!(&quot;{}&quot;, value);
    } else {
        println!(&quot;NULL&quot;);
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>Like <code>FromIterator</code>, this crate contains two sets of APIs to iterate over data. Given
an array <code>array: &amp;PrimitiveArray&lt;T&gt;</code>, the following applies:</p>
<ol>
<li>If you need to iterate over <code>Option&lt;&amp;T&gt;</code>, use <code>array.iter()</code></li>
<li>If you can operate over the values and validity independently,
use <code>array.values() -&gt; &amp;Buffer&lt;T&gt;</code> and <code>array.validity() -&gt; Option&lt;&amp;Bitmap&gt;</code></li>
</ol>
<p>Note that case 1 is useful when e.g. you want to perform an operation that depends on both
validity and values, while the latter is suitable for SIMD and copies, as they return
contiguous memory regions (buffers and bitmaps). We will see below how to leverage these APIs.</p>
<p>This idea holds more generally in this crate's arrays: <code>values()</code> returns something that has
a contiguous in-memory representation, while <code>iter()</code> returns items taking validity into account. 
To get an iterator over contiguous values, use <code>array.values().iter()</code>.</p>
<p>There is one last API that is worth mentioning, and that is <code>Bitmap::chunks</code>. When performing
bitwise operations, it is often more performant to operate on chunks of bits
instead of single bits. <code>chunks</code> offers a <code>TrustedLen</code> of <code>u64</code> with the bits
+ an extra <code>u64</code> remainder. We expose two functions, <code>unary(Bitmap, Fn) -&gt; Bitmap</code>
and <code>binary(Bitmap, Bitmap, Fn) -&gt; Bitmap</code> that use this API to efficiently
perform bitmap operations.</p>
<h2 id="vectorized-operations"><a class="header" href="#vectorized-operations">Vectorized operations</a></h2>
<p>One of the main advantages of the arrow format and its memory layout is that
it often enables SIMD. For example, an unary operation <code>op</code> on a <code>PrimitiveArray</code>
likely emits SIMD instructions on the following code:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use arrow2::buffer::Buffer;
</span><span class="boring">use arrow2::{
</span><span class="boring">    array::{Array, PrimitiveArray},
</span><span class="boring">    types::NativeType,
</span><span class="boring">    datatypes::DataType,
</span><span class="boring">};
</span>
pub fn unary&lt;I, F, O&gt;(array: &amp;PrimitiveArray&lt;I&gt;, op: F, data_type: &amp;DataType) -&gt; PrimitiveArray&lt;O&gt;
where
    I: NativeType,
    O: NativeType,
    F: Fn(I) -&gt; O,
{
    // create the iterator over _all_ values
    let values = array.values().iter().map(|v| op(*v));
    let values = Buffer::from_trusted_len_iter(values);

    // create the new array, cloning its validity
    PrimitiveArray::&lt;O&gt;::from_data(data_type.clone(), values, array.validity().cloned())
}
<span class="boring">}
</span></code></pre></pre>
<p>Some notes:</p>
<ol>
<li>
<p>We used <code>array.values()</code>, as described above: this operation leverages a
contiguous memory region.</p>
</li>
<li>
<p>We leveraged normal rust iterators for the operation.</p>
</li>
<li>
<p>We used <code>op</code> on the array's values irrespectively of their validity,
and cloned its validity. This approach is suitable for operations whose branching off
is more expensive than operating over all values. If the operation is expensive,
then using <code>PrimitiveArray::&lt;O&gt;::from_trusted_len_iter</code> is likely faster.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compute-api"><a class="header" href="#compute-api">Compute API</a></h1>
<p>When compiled with the feature <code>compute</code>, this crate offers a wide range of functions
to perform both vertical (e.g. add two arrays) and horizontal
(compute the sum of an array) operations.</p>
<p>The overall design of the <code>compute</code> module is that it offers two APIs:</p>
<ul>
<li>statically typed, such as <code>sum_primitive&lt;T&gt;(&amp;PrimitiveArray&lt;T&gt;) -&gt; Option&lt;T&gt;</code></li>
<li>dynamically typed, such as <code>sum(&amp;dyn Array) -&gt; Box&lt;dyn Scalar&gt;</code></li>
</ul>
<p>the dynamically typed API usually has a function <code>can_*(&amp;DataType) -&gt; bool</code> denoting whether
the operation is defined for the particular logical type.</p>
<p>Overview of the implemented functionality:</p>
<ul>
<li>arithmetics, checked, saturating, etc.</li>
<li><code>sum</code>, <code>min</code> and <code>max</code></li>
<li><code>unary</code>, <code>binary</code>, etc.</li>
<li><code>comparison</code></li>
<li><code>cast</code></li>
<li><code>take</code>, <code>filter</code>, <code>concat</code></li>
<li><code>sort</code>, <code>hash</code>, <code>merge-sort</code></li>
<li><code>if-then-else</code></li>
<li><code>nullif</code></li>
<li><code>length</code> (of string)</li>
<li><code>hour</code>, <code>year</code>, <code>month</code>, <code>iso_week</code> (of temporal logical types)</li>
<li><code>regex</code></li>
<li>(list) <code>contains</code></li>
</ul>
<p>and an example of how to use them:</p>
<pre><pre class="playground"><code class="language-rust">use arrow2::array::{Array, PrimitiveArray};
use arrow2::compute::arithmetics::*;
use arrow2::compute::arity::{binary, unary};
use arrow2::datatypes::DataType;
use arrow2::error::Result;

fn main() -&gt; Result&lt;()&gt; {
    // say we have two arrays
    let array0 = PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(1), Some(2), Some(3)]);
    let array1 = PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(4), None, Some(6)]);

    // we can add them as follows:
    let added = arithmetic_primitive(&amp;array0, Operator::Add, &amp;array1)?;
    assert_eq!(
        added,
        PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(5), None, Some(9)])
    );

    // subtract:
    let subtracted = arithmetic_primitive(&amp;array0, Operator::Subtract, &amp;array1)?;
    assert_eq!(
        subtracted,
        PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(-3), None, Some(-3)])
    );

    // add a scalar:
    let plus10 = arithmetic_primitive_scalar(&amp;array0, Operator::Add, &amp;10)?;
    assert_eq!(
        plus10,
        PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(11), Some(12), Some(13)])
    );

    // when the array is a trait object, there is a similar API
    let array0 = &amp;array0 as &amp;dyn Array;
    let array1 = &amp;array1 as &amp;dyn Array;

    // check whether the logical types support addition (they could be any `Array`).
    assert!(can_arithmetic(
        array0.data_type(),
        Operator::Add,
        array1.data_type()
    ));

    // add them
    let added = arithmetic(array0, Operator::Add, array1).unwrap();
    assert_eq!(
        PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(5), None, Some(9)]),
        added.as_ref(),
    );

    // a more exotic implementation: arbitrary binary operations
    // this is compiled to SIMD when intrinsics exist.
    let array0 = PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(1), Some(2), Some(3)]);
    let array1 = PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(4), None, Some(6)]);

    let op = |x: i64, y: i64| x.pow(2) + y.pow(2);
    let r = binary(&amp;array0, &amp;array1, DataType::Int64, op)?;
    assert_eq!(
        r,
        PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(1 + 16), None, Some(9 + 36)])
    );

    // arbitrary unary operations
    // this is compiled to SIMD when intrinsics exist.
    let array0 = PrimitiveArray::&lt;f64&gt;::from(&amp;[Some(4.0), None, Some(6.0)]);
    let r = unary(
        &amp;array0,
        |x| x.cos().powi(2) + x.sin().powi(2),
        DataType::Float64,
    );
    assert!((r.values()[0] - 1.0).abs() &lt; 0.0001);
    assert!(r.is_null(1));
    assert!((r.values()[2] - 1.0).abs() &lt; 0.0001);

    // finally, a transformation that changes types:
    let array0 = PrimitiveArray::&lt;f64&gt;::from(&amp;[Some(4.4), None, Some(4.6)]);
    let rounded = unary(&amp;array0, |x| x.round() as i64, DataType::Int64);
    assert_eq!(
        rounded,
        PrimitiveArray::&lt;i64&gt;::from(&amp;[Some(4), None, Some(5)])
    );

    Ok(())
}
</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metadata"><a class="header" href="#metadata">Metadata</a></h1>
<pre><pre class="playground"><code class="language-rust">use std::collections::{BTreeMap, HashMap};

use arrow2::datatypes::{DataType, Field, Schema};

fn main() {
    // two data types (logical types)
    let type1_ = DataType::Date32;
    let type2_ = DataType::Int32;

    // two fields (columns)
    let field1 = Field::new(&quot;c1&quot;, type1_, true);
    let field2 = Field::new(&quot;c2&quot;, type2_, true);

    // which can contain extra metadata:
    let mut metadata = BTreeMap::new();
    metadata.insert(
        &quot;Office Space&quot;.to_string(),
        &quot;Deals with real issues in the workplace.&quot;.to_string(),
    );
    let field1 = field1.with_metadata(metadata);

    // a schema (a table)
    let schema = Schema::new(vec![field1, field2]);

    assert_eq!(schema.fields().len(), 2);

    // which can also contain extra metadata:
    let mut metadata = HashMap::new();
    metadata.insert(
        &quot;Office Space&quot;.to_string(),
        &quot;Deals with real issues in the workplace.&quot;.to_string(),
    );
    let schema = schema.with_metadata(metadata);

    assert_eq!(schema.fields().len(), 2);
}
</code></pre></pre>
<h2 id="datatype-logical-types"><a class="header" href="#datatype-logical-types"><code>DataType</code> (Logical types)</a></h2>
<p>The Arrow specification contains a set of logical types, an enumeration of the different
semantical types defined in Arrow.</p>
<p>In Arrow2, logical types are declared as variants of the <code>enum</code> <code>arrow2::datatypes::DataType</code>.
For example, <code>DataType::Int32</code> represents a signed integer of 32 bits.</p>
<p>Each <code>DataType</code> has an associated <code>enum PhysicalType</code> (many-to-one) representing the
particular in-memory representation, and is associated to a specific semantics.
For example, both <code>DataType::Date32</code> and <code>DataType::Int32</code> have the same <code>PhysicalType</code>
(<code>PhysicalType::Primitive(PrimitiveType::Int32)</code>) but <code>Date32</code> represents the number of
days since UNIX epoch.</p>
<p>Logical types are metadata: they annotate physical types with extra information about data.</p>
<h2 id="field-column-metadata"><a class="header" href="#field-column-metadata"><code>Field</code> (column metadata)</a></h2>
<p>Besides logical types, the arrow format supports other relevant metadata to the format.
An important one is <code>Field</code> broadly corresponding to a column in traditional columnar formats.
A <code>Field</code> is composed by a name (<code>String</code>), a logical type (<code>DataType</code>), whether it is
nullable (<code>bool</code>), and optional metadata.</p>
<h2 id="schema-table-metadata"><a class="header" href="#schema-table-metadata"><code>Schema</code> (table metadata)</a></h2>
<p>The most common use of <code>Field</code> is to declare a <code>arrow2::datatypes::Schema</code>, a sequence of <code>Field</code>s
with optional metadata.</p>
<p><code>Schema</code> is essentially metadata of a &quot;table&quot;: it has a sequence of named columns and their metadata (<code>Field</code>s) with optional metadata.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="foreign-interfaces"><a class="header" href="#foreign-interfaces">Foreign Interfaces</a></h1>
<p>One of the hallmarks of the Arrow format is that its in-memory representation
has a specification, which allows languages to share data
structures via foreign interfaces at zero cost (i.e. via pointers).
This is known as the <a href="https://arrow.apache.org/docs/format/CDataInterface.html">C Data interface</a>.</p>
<p>This crate supports importing from and exporting to all its physical types. The
example below demonstrates how to use the API:</p>
<pre><pre class="playground"><code class="language-rust">use arrow2::array::{Array, PrimitiveArray};
use arrow2::datatypes::Field;
use arrow2::error::Result;
use arrow2::ffi;
use std::sync::Arc;

unsafe fn export(
    array: Arc&lt;dyn Array&gt;,
    array_ptr: *mut ffi::Ffi_ArrowArray,
    schema_ptr: *mut ffi::Ffi_ArrowSchema,
) {
    let field = Field::new(&quot;a&quot;, array.data_type().clone(), true);
    ffi::export_array_to_c(array, array_ptr);
    ffi::export_field_to_c(&amp;field, schema_ptr);
}

unsafe fn import(
    array: Box&lt;ffi::Ffi_ArrowArray&gt;,
    schema: &amp;ffi::Ffi_ArrowSchema,
) -&gt; Result&lt;Box&lt;dyn Array&gt;&gt; {
    let field = ffi::import_field_from_c(schema)?;
    ffi::import_array_from_c(array, &amp;field)
}

fn main() -&gt; Result&lt;()&gt; {
    // let's assume that we have an array:
    let array = Arc::new(PrimitiveArray::&lt;i32&gt;::from([Some(1), None, Some(123)])) as Arc&lt;dyn Array&gt;;

    // the goal is to export this array and import it back via FFI.
    // to import, we initialize the structs that will receive the data
    let array_ptr = Box::new(ffi::Ffi_ArrowArray::empty());
    let schema_ptr = Box::new(ffi::Ffi_ArrowSchema::empty());

    // since FFIs work in raw pointers, let's temporarily relinquish ownership so that producers
    // can write into it in a thread-safe manner
    let array_ptr = Box::into_raw(array_ptr);
    let schema_ptr = Box::into_raw(schema_ptr);

    // this is where a producer (in this case also us ^_^) writes to the pointers' location.
    // `array` here could be anything or not even be available, if this was e.g. from Python.
    // Safety: we just allocated the pointers correctly.
    unsafe { export(array.clone(), array_ptr, schema_ptr) };

    // we can now take ownership back, since we are responsible for deallocating this memory.
    // Safety: we just into_raw them.
    let array_ptr = unsafe { Box::from_raw(array_ptr) };
    let schema_ptr = unsafe { Box::from_raw(schema_ptr) };

    // and finally interpret the written memory into a new array.
    // Safety: we used `export`, which is a valid exporter to the C data interface
    let new_array = unsafe { import(array_ptr, schema_ptr.as_ref())? };

    // which is equal to the exported array
    assert_eq!(array.as_ref(), new_array.as_ref());
    Ok(())
}
</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="extension-types"><a class="header" href="#extension-types">Extension types</a></h1>
<p>This crate supports Arrows' <a href="https://arrow.apache.org/docs/format/Columnar.html#extension-types">&quot;extension type&quot;</a>, to declare, use, and share custom logical types.</p>
<p>An extension type is just a <code>DataType</code> with a name and some metadata.
In particular, its physical representation is equal to its inner <code>DataType</code>, which implies
that all functionality in this crate works as if it was the inner <code>DataType</code>.</p>
<p>The following example shows how to declare one:</p>
<pre><pre class="playground"><code class="language-rust">use std::io::{Cursor, Seek, Write};
use std::sync::Arc;

use arrow2::array::*;
use arrow2::datatypes::*;
use arrow2::error::Result;
use arrow2::io::ipc::read;
use arrow2::io::ipc::write;
use arrow2::record_batch::RecordBatch;

fn main() -&gt; Result&lt;()&gt; {
    // declare an extension.
    let extension_type =
        DataType::Extension(&quot;date16&quot;.to_string(), Box::new(DataType::UInt16), None);

    // initialize an array with it.
    let array = UInt16Array::from_slice([1, 2]).to(extension_type.clone());

    // from here on, it works as usual
    let buffer = Cursor::new(vec![]);

    // write to IPC
    let result_buffer = write_ipc(buffer, array)?;

    // read it back
    let batch = read_ipc(&amp;result_buffer.into_inner())?;

    // and verify that the datatype is preserved.
    let array = &amp;batch.columns()[0];
    assert_eq!(array.data_type(), &amp;extension_type);

    // see https://arrow.apache.org/docs/format/Columnar.html#extension-types
    // for consuming by other consumers.
    Ok(())
}

fn write_ipc&lt;W: Write + Seek&gt;(writer: W, array: impl Array + 'static) -&gt; Result&lt;W&gt; {
    let schema = Schema::new(vec![Field::new(&quot;a&quot;, array.data_type().clone(), false)]);

    let mut writer = write::FileWriter::try_new(writer, &amp;schema)?;

    let batch = RecordBatch::try_new(Arc::new(schema), vec![Arc::new(array)])?;

    writer.write(&amp;batch)?;

    Ok(writer.into_inner())
}

fn read_ipc(buf: &amp;[u8]) -&gt; Result&lt;RecordBatch&gt; {
    let mut cursor = Cursor::new(buf);
    let metadata = read::read_file_metadata(&amp;mut cursor)?;
    let mut reader = read::FileReader::new(cursor, metadata, None);
    reader.next().unwrap()
}
</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="io"><a class="header" href="#io">IO</a></h1>
<p>This crate offers optional features that enable interoperability with different formats:</p>
<ul>
<li>Arrow (<code>io_ipc</code>)</li>
<li>CSV (<code>io_csv</code>)</li>
<li>Parquet (<code>io_parquet</code>)</li>
<li>Json (<code>io_json</code>)</li>
</ul>
<p>In this section you can find a guide and examples for each one of them.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="csv-reader"><a class="header" href="#csv-reader">CSV reader</a></h1>
<p>When compiled with feature <code>io_csv</code>, you can use this crate to read CSV files.
This crate makes minimal assumptions on how you want to read a CSV, and offers a large degree of customization to it, along with a useful default.</p>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>There are two CPU-intensive tasks in reading a CSV file:</p>
<ul>
<li>split the CSV file into rows, which includes parsing quotes and delimiters, and is necessary to <code>seek</code> to a given row.</li>
<li>parse a set of CSV rows (bytes) into a <code>RecordBatch</code>.</li>
</ul>
<p>Parsing bytes into values is more expensive than interpreting lines. As such, it is generally advantageous to have multiple readers of a single file that scan different parts of the file (within IO constraints).</p>
<p>This crate relies on <a href="https://crates.io/crates/csv">the crate <code>csv</code></a> to scan and seek CSV files, and your code also needs such a dependency. With that said, <code>arrow2</code> makes no assumptions as to how to efficiently read the CSV: as a single reader per file or multiple readers.</p>
<p>As an example, the following infers the schema and reads a CSV by re-using the same reader:</p>
<pre><pre class="playground"><code class="language-rust">use arrow2::error::Result;
use arrow2::io::csv::read;
use arrow2::record_batch::RecordBatch;

fn read_path(path: &amp;str, projection: Option&lt;&amp;[usize]&gt;) -&gt; Result&lt;RecordBatch&gt; {
    // Create a CSV reader. This is typically created on the thread that reads the file and
    // thus owns the read head.
    let mut reader = read::ReaderBuilder::new().from_path(path)?;

    // Infers the schema using the default inferer. The inferer is just a function that maps a string
    // to a `DataType`.
    let schema = read::infer_schema(&amp;mut reader, None, true, &amp;read::infer)?;

    // allocate space to read from CSV to. The size of this vec denotes how many rows are read.
    let mut rows = vec![read::ByteRecord::default(); 100];

    // skip 0 (excluding the header) and read up to 100 rows.
    // this is IO-intensive and performs minimal CPU work. In particular,
    // no deserialization is performed.
    let rows_read = read::read_rows(&amp;mut reader, 0, &amp;mut rows)?;
    let rows = &amp;rows[..rows_read];

    // parse the batches into a `RecordBatch`. This is CPU-intensive, has no IO,
    // and can be performed on a different thread by passing `rows` through a channel.
    read::deserialize_batch(
        rows,
        schema.fields(),
        projection,
        0,
        read::deserialize_column,
    )
}

fn main() -&gt; Result&lt;()&gt; {
    use std::env;
    let args: Vec&lt;String&gt; = env::args().collect();

    let file_path = &amp;args[1];

    let batch = read_path(file_path, None)?;
    println!(&quot;{:?}&quot;, batch);
    Ok(())
}
</code></pre></pre>
<h2 id="orchestration-and-parallelization"><a class="header" href="#orchestration-and-parallelization">Orchestration and parallelization</a></h2>
<p>Because <code>csv</code>'s API is synchronous, the functions above represent the &quot;minimal
unit of synchronous work&quot;, IO and CPU. Note that <code>rows</code> above are <code>Send</code>,
which implies that it is possible to run <code>parse</code> on a separate thread,
thereby maximizing IO throughput. The example below shows how to do just that:</p>
<pre><pre class="playground"><code class="language-rust">use crossbeam_channel::unbounded;

use std::sync::Arc;
use std::thread;
use std::time::SystemTime;

use arrow2::{error::Result, io::csv::read, record_batch::RecordBatch};

fn parallel_read(path: &amp;str) -&gt; Result&lt;Vec&lt;RecordBatch&gt;&gt; {
    let batch_size = 100;
    let has_header = true;
    let projection = None;

    // prepare a channel to send serialized records from threads
    let (tx, rx) = unbounded();

    let mut reader = read::ReaderBuilder::new().from_path(path)?;
    let schema = read::infer_schema(&amp;mut reader, Some(batch_size * 10), has_header, &amp;read::infer)?;
    let schema = Arc::new(schema);

    let start = SystemTime::now();
    // spawn a thread to produce `Vec&lt;ByteRecords&gt;` (IO bounded)
    let child = thread::spawn(move || {
        let mut line_number = 0;
        let mut size = 1;
        while size &gt; 0 {
            let mut rows = vec![read::ByteRecord::default(); batch_size];
            let rows_read = read::read_rows(&amp;mut reader, 0, &amp;mut rows).unwrap();
            rows.truncate(rows_read);
            line_number += rows.len();
            size = rows.len();
            tx.send((rows, line_number)).unwrap();
        }
    });

    let mut children = Vec::new();
    // use 3 consumers of to decompress, decode and deserialize.
    for _ in 0..3 {
        let rx_consumer = rx.clone();
        let consumer_schema = schema.clone();
        let child = thread::spawn(move || {
            let (rows, line_number) = rx_consumer.recv().unwrap();
            let start = SystemTime::now();
            println!(&quot;consumer start - {}&quot;, line_number);
            let batch = read::deserialize_batch(
                &amp;rows,
                consumer_schema.fields(),
                projection,
                0,
                read::deserialize_column,
            )
            .unwrap();
            println!(
                &quot;consumer end - {:?}: {}&quot;,
                start.elapsed().unwrap(),
                line_number,
            );
            batch
        });
        children.push(child);
    }

    child.join().expect(&quot;child thread panicked&quot;);

    let batches = children
        .into_iter()
        .map(|x| x.join().unwrap())
        .collect::&lt;Vec&lt;_&gt;&gt;();
    println!(&quot;Finished - {:?}&quot;, start.elapsed().unwrap());

    Ok(batches)
}

fn main() -&gt; Result&lt;()&gt; {
    use std::env;
    let args: Vec&lt;String&gt; = env::args().collect();
    let file_path = &amp;args[1];

    let batches = parallel_read(file_path)?;
    for batch in batches {
        println!(&quot;{}&quot;, batch.num_rows())
    }
    Ok(())
}
</code></pre></pre>
<h2 id="async"><a class="header" href="#async">Async</a></h2>
<p>This crate also supports reading from a CSV asyncronously through the <code>csv-async</code> crate.
The example below demonstrates this:</p>
<pre><pre class="playground"><code class="language-rust">use std::sync::Arc;

use futures::io::Cursor;
use tokio::fs::File;
use tokio_util::compat::*;

use arrow2::array::*;
use arrow2::error::Result;
use arrow2::io::csv::read_async::*;

#[tokio::main(flavor = &quot;current_thread&quot;)]
async fn main() -&gt; Result&lt;()&gt; {
    use std::env;
    let args: Vec&lt;String&gt; = env::args().collect();

    let file_path = &amp;args[1];

    let file = File::open(file_path).await?.compat();

    let mut reader = AsyncReaderBuilder::new().create_reader(file);

    let schema = Arc::new(infer_schema(&amp;mut reader, None, true, &amp;infer).await?);

    let mut rows = vec![ByteRecord::default(); 100];
    let rows_read = read_rows(&amp;mut reader, 0, &amp;mut rows).await?;

    let batch = deserialize_batch(
        &amp;rows[..rows_read],
        schema.fields(),
        None,
        0,
        deserialize_column,
    )?;
    println!(&quot;{}&quot;, batch.column(0));
    Ok(())
}
</code></pre></pre>
<p>Note that the deserialization <em>should</em> be performed on a separate thread to not
block (see also <a href="https://ryhl.io/blog/async-what-is-blocking/">here</a>), which this
example does not show.</p>
<h2 id="customization"><a class="header" href="#customization">Customization</a></h2>
<p>In the code above, <code>parser</code> and <code>infer</code> allow for customization: they declare
how rows of bytes should be inferred (into a logical type), and processed (into a value of said type).
They offer good default options, but you can customize the inference and parsing to your own needs.
You can also of course decide to parse everything into memory as <code>Utf8Array</code> and
delay any data transformation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="write-csv"><a class="header" href="#write-csv">Write CSV</a></h1>
<p>When compiled with feature <code>io_csv</code>, you can use this crate to write CSV files.</p>
<p>This crate relies on <a href="https://crates.io/crates/csv">the crate csv</a> to write well-formed CSV files, which your code should also depend on.</p>
<p>The following example writes a batch as a CSV file with the default configuration:</p>
<pre><pre class="playground"><code class="language-rust">use std::sync::Arc;

use arrow2::{
    array::{Array, Int32Array},
    datatypes::{Field, Schema},
    error::Result,
    io::csv::write,
    record_batch::RecordBatch,
};

fn write_batch(path: &amp;str, batches: &amp;[RecordBatch]) -&gt; Result&lt;()&gt; {
    let writer = &amp;mut write::WriterBuilder::new().from_path(path)?;

    write::write_header(writer, batches[0].schema())?;

    let options = write::SerializeOptions::default();
    batches
        .iter()
        .try_for_each(|batch| write::write_batch(writer, batch, &amp;options))
}

fn main() -&gt; Result&lt;()&gt; {
    let array = Int32Array::from(&amp;[
        Some(0),
        Some(1),
        Some(2),
        Some(3),
        Some(4),
        Some(5),
        Some(6),
    ]);
    let field = Field::new(&quot;c1&quot;, array.data_type().clone(), true);
    let schema = Schema::new(vec![field]);
    let batch = RecordBatch::try_new(Arc::new(schema), vec![Arc::new(array)])?;

    write_batch(&quot;example.csv&quot;, &amp;[batch])
}
</code></pre></pre>
<h2 id="parallelism"><a class="header" href="#parallelism">Parallelism</a></h2>
<p>This crate exposes functionality to decouple serialization from writing.</p>
<p>In the example above, the serialization and writing to a file is done synchronously.
However, these typically deal with different bounds: serialization is often CPU bounded, while writing is often IO bounded. We can trade-off these through a higher memory usage.</p>
<p>Suppose that we know that we are getting CPU-bounded at serialization, and would like to offload that workload to other threads, at the cost of a higher memory usage. We would achieve this as follows (two batches for simplicity):</p>
<pre><pre class="playground"><code class="language-rust">use std::sync::mpsc;
use std::sync::mpsc::{Receiver, Sender};
use std::sync::Arc;
use std::thread;

use arrow2::{
    array::{Array, Int32Array},
    datatypes::{Field, Schema},
    error::Result,
    io::csv::write,
    record_batch::RecordBatch,
};

fn parallel_write(path: &amp;str, batches: [RecordBatch; 2]) -&gt; Result&lt;()&gt; {
    let options = write::SerializeOptions::default();

    // write a header
    let writer = &amp;mut write::WriterBuilder::new().from_path(path)?;
    write::write_header(writer, batches[0].schema())?;

    // prepare a channel to send serialized records from threads
    let (tx, rx): (Sender&lt;_&gt;, Receiver&lt;_&gt;) = mpsc::channel();
    let mut children = Vec::new();

    (0..2).for_each(|id| {
        // The sender endpoint can be cloned
        let thread_tx = tx.clone();

        let options = options.clone();
        let batch = batches[id].clone(); // note: this is cheap
        let child = thread::spawn(move || {
            let records = write::serialize(&amp;batch, &amp;options).unwrap();
            thread_tx.send(records).unwrap();
        });

        children.push(child);
    });

    for _ in 0..2 {
        // block: assumes that the order of batches matter.
        let records = rx.recv().unwrap();
        records
            .iter()
            .try_for_each(|record| writer.write_byte_record(record))?
    }

    for child in children {
        child.join().expect(&quot;child thread panicked&quot;);
    }

    Ok(())
}

fn main() -&gt; Result&lt;()&gt; {
    let array = Int32Array::from(&amp;[
        Some(0),
        Some(1),
        Some(2),
        Some(3),
        Some(4),
        Some(5),
        Some(6),
    ]);
    let field = Field::new(&quot;c1&quot;, array.data_type().clone(), true);
    let schema = Schema::new(vec![field]);
    let batch = RecordBatch::try_new(Arc::new(schema), vec![Arc::new(array)])?;

    parallel_write(&quot;example.csv&quot;, [batch.clone(), batch])
}
</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="read-parquet"><a class="header" href="#read-parquet">Read parquet</a></h1>
<p>When compiled with feature <code>io_parquet</code>, this crate can be used to read parquet files
to arrow.
It makes minimal assumptions on how you to decompose CPU and IO intensive tasks.</p>
<p>First, some notation:</p>
<ul>
<li><code>page</code>: part of a column (e.g. similar of a slice of an <code>Array</code>)</li>
<li><code>column chunk</code>: composed of multiple pages (similar of an <code>Array</code>)</li>
<li><code>row group</code>: a group of columns with the same length (similar of a <code>RecordBatch</code> in Arrow)</li>
</ul>
<p>Here is how to read a single column chunk from a single row group:</p>
<pre><pre class="playground"><code class="language-rust">use std::fs::File;
use std::io::BufReader;

use arrow2::io::parquet::read;
use arrow2::{array::Array, error::Result};

fn read_column_chunk(path: &amp;str, row_group: usize, column: usize) -&gt; Result&lt;Box&lt;dyn Array&gt;&gt; {
    // Open a file, a common operation in Rust
    let mut file = BufReader::new(File::open(path)?);

    // Read the files' metadata. This has a small IO cost because it requires seeking to the end
    // of the file to read its footer.
    let file_metadata = read::read_metadata(&amp;mut file)?;

    // Convert the files' metadata into an arrow schema. This is CPU-only and amounts to
    // parse thrift if the arrow format is available on a key, or infering the arrow schema from
    // the parquet's physical, converted and logical types.
    let arrow_schema = read::get_schema(&amp;file_metadata)?;

    // get the columns' metadata
    let metadata = file_metadata.row_groups[row_group].column(column);

    // Construct an iterator over pages. This binds `file` to this iterator, and each iteration
    // is IO intensive as it will read a compressed page into memory. There is almost no CPU work
    // on this operation
    let pages = read::get_page_iterator(metadata, &amp;mut file, None, vec![])?;

    // get the columns' logical type
    let data_type = arrow_schema.fields()[column].data_type().clone();

    // This is the actual work. In this case, pages are read (by calling `iter.next()`) and are
    // immediately decompressed, decoded, deserialized to arrow and deallocated.
    // This uses a combination of IO and CPU. At this point, `array` is the arrow-corresponding
    // array of the parquets' physical type.
    // `Decompressor` re-uses an internal buffer for de-compression, thereby maximizing memory re-use.
    let mut pages = read::Decompressor::new(pages, vec![]);

    read::page_iter_to_array(&amp;mut pages, metadata, data_type)
}

fn main() -&gt; Result&lt;()&gt; {
    use std::env;
    let args: Vec&lt;String&gt; = env::args().collect();

    let file_path = &amp;args[1];
    let column = args[2].parse::&lt;usize&gt;().unwrap();
    let row_group = args[3].parse::&lt;usize&gt;().unwrap();

    let array = read_column_chunk(file_path, row_group, column)?;
    println!(&quot;{}&quot;, array);
    Ok(())
}
</code></pre></pre>
<p>The example above minimizes memory usage at the expense of mixing IO and CPU tasks
on the same thread, which may hurt performance if one of them is a bottleneck.</p>
<p>For single-threaded reading, buffers used to read and decompress pages can be re-used.
This create offers an API that encapsulates the above logic:</p>
<pre><pre class="playground"><code class="language-rust">use std::fs::File;

use arrow2::error::Result;
use arrow2::io::parquet::read;

fn main() -&gt; Result&lt;()&gt; {
    use std::env;
    let args: Vec&lt;String&gt; = env::args().collect();

    let file_path = &amp;args[1];

    let reader = File::open(file_path)?;
    let reader = read::RecordReader::try_new(reader, None, None, None, None)?;

    for maybe_batch in reader {
        let batch = maybe_batch?;
        println!(&quot;{:?}&quot;, batch);
    }
    Ok(())
}
</code></pre></pre>
<h3 id="parallelism-decoupling-of-cpu-from-io"><a class="header" href="#parallelism-decoupling-of-cpu-from-io">Parallelism decoupling of CPU from IO</a></h3>
<p>One important aspect of the pages created by the iterator above is that they can cross
thread boundaries. Consequently, the thread reading pages from a file (IO-bounded)
does not have to be the same thread performing CPU-bounded work (decompressing,
decoding, etc.).</p>
<p>The example below assumes that CPU starves the consumption of pages,
and that it is advantageous to have a single thread performing all IO-intensive work,
by delegating all CPU-intensive tasks to separate threads.</p>
<pre><pre class="playground"><code class="language-rust">use crossbeam_channel::unbounded;

use std::fs::File;
use std::sync::Arc;
use std::thread;
use std::time::SystemTime;

use arrow2::{array::Array, error::Result, io::parquet::read};

fn parallel_read(path: &amp;str) -&gt; Result&lt;Vec&lt;Box&lt;dyn Array&gt;&gt;&gt; {
    // prepare a channel to send serialized records from threads
    let (tx, rx) = unbounded();

    let mut file = File::open(path)?;
    let file_metadata = read::read_metadata(&amp;mut file)?;
    let arrow_schema = Arc::new(read::get_schema(&amp;file_metadata)?);

    let file_metadata = Arc::new(file_metadata);

    let start = SystemTime::now();
    // spawn a thread to produce `Vec&lt;CompressedPage&gt;` (IO bounded)
    let producer_metadata = file_metadata.clone();
    let child = thread::spawn(move || {
        for column in 0..producer_metadata.schema().num_columns() {
            for row_group in 0..producer_metadata.row_groups.len() {
                let start = SystemTime::now();
                let column_metadata = producer_metadata.row_groups[row_group].column(column);
                println!(&quot;produce start: {} {}&quot;, column, row_group);
                let pages = read::get_page_iterator(column_metadata, &amp;mut file, None, vec![])
                    .unwrap()
                    .collect::&lt;Vec&lt;_&gt;&gt;();
                println!(
                    &quot;produce end - {:?}: {} {}&quot;,
                    start.elapsed().unwrap(),
                    column,
                    row_group
                );
                tx.send((column, row_group, pages)).unwrap();
            }
        }
    });

    let mut children = Vec::new();
    // use 3 consumers of to decompress, decode and deserialize.
    for _ in 0..3 {
        let rx_consumer = rx.clone();
        let metadata_consumer = file_metadata.clone();
        let arrow_schema_consumer = arrow_schema.clone();
        let child = thread::spawn(move || {
            let (column, row_group, pages) = rx_consumer.recv().unwrap();
            let start = SystemTime::now();
            println!(&quot;consumer start - {} {}&quot;, column, row_group);
            let metadata = metadata_consumer.row_groups[row_group].column(column);
            let data_type = arrow_schema_consumer.fields()[column].data_type().clone();

            let mut pages = read::BasicDecompressor::new(pages.into_iter(), vec![]);

            let array = read::page_iter_to_array(&amp;mut pages, metadata, data_type);
            println!(
                &quot;consumer end - {:?}: {} {}&quot;,
                start.elapsed().unwrap(),
                column,
                row_group
            );
            array
        });
        children.push(child);
    }

    child.join().expect(&quot;child thread panicked&quot;);

    let arrays = children
        .into_iter()
        .map(|x| x.join().unwrap())
        .collect::&lt;Result&lt;Vec&lt;_&gt;&gt;&gt;()?;
    println!(&quot;Finished - {:?}&quot;, start.elapsed().unwrap());

    Ok(arrays)
}

fn main() -&gt; Result&lt;()&gt; {
    use std::env;
    let args: Vec&lt;String&gt; = env::args().collect();
    let file_path = &amp;args[1];

    let arrays = parallel_read(file_path)?;
    for array in arrays {
        println!(&quot;{}&quot;, array)
    }
    Ok(())
}
</code></pre></pre>
<p>This can of course be reversed; in configurations where IO is bounded (e.g. when a
network is involved), we can use multiple producers of pages, potentially divided
in file readers, and a single consumer that performs all CPU-intensive work.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="write-to-parquet"><a class="header" href="#write-to-parquet">Write to Parquet</a></h1>
<p>When compiled with feature <code>io_parquet</code>, this crate can be used to write parquet files
from arrow.
It makes minimal assumptions on how you to decompose CPU and IO intensive tasks, as well
as an higher-level API to abstract away some of this work into an easy to use API.</p>
<p>First, some notation:</p>
<ul>
<li><code>page</code>: part of a column (e.g. similar of a slice of an <code>Array</code>)</li>
<li><code>column chunk</code>: composed of multiple pages (similar of an <code>Array</code>)</li>
<li><code>row group</code>: a group of columns with the same length (similar of a <code>RecordBatch</code> in Arrow)</li>
</ul>
<h2 id="single-threaded"><a class="header" href="#single-threaded">Single threaded</a></h2>
<p>Here is an example of how to write a single column chunk into a single row group:</p>
<pre><pre class="playground"><code class="language-rust">use std::fs::File;
use std::iter::once;

use arrow2::error::ArrowError;
use arrow2::io::parquet::write::to_parquet_schema;
use arrow2::{
    array::{Array, Int32Array},
    datatypes::{Field, Schema},
    error::Result,
    io::parquet::write::{
        array_to_pages, write_file, Compression, Compressor, DynIter, DynStreamingIterator,
        Encoding, FallibleStreamingIterator, Version, WriteOptions,
    },
};

fn write_single_array(path: &amp;str, array: &amp;dyn Array, field: Field) -&gt; Result&lt;()&gt; {
    let schema = Schema::new(vec![field]);

    let options = WriteOptions {
        write_statistics: true,
        compression: Compression::Uncompressed,
        version: Version::V2,
    };
    let encoding = Encoding::Plain;

    // map arrow fields to parquet fields
    let parquet_schema = to_parquet_schema(&amp;schema)?;

    let descriptor = parquet_schema.columns()[0].clone();

    // Declare the row group iterator. This must be an iterator of iterators of streaming iterators
    // * first iterator over row groups
    let row_groups = once(Result::Ok(DynIter::new(
        // * second iterator over column chunks (we assume no struct arrays -&gt; `once` column)
        once(
            // * third iterator over (compressed) pages; dictionary encoding may lead to multiple pages per array.
            array_to_pages(array, descriptor, options, encoding).map(move |pages| {
                let encoded_pages = DynIter::new(pages.map(|x| Ok(x?)));
                let compressed_pages = Compressor::new(encoded_pages, options.compression, vec![])
                    .map_err(ArrowError::from);
                DynStreamingIterator::new(compressed_pages)
            }),
        ),
    )));

    // Create a new empty file
    let mut file = File::create(path)?;

    // Write the file. Note that, at present, any error results in a corrupted file.
    let _ = write_file(
        &amp;mut file,
        row_groups,
        &amp;schema,
        parquet_schema,
        options,
        None,
    )?;
    Ok(())
}

fn main() -&gt; Result&lt;()&gt; {
    let array = Int32Array::from(&amp;[
        Some(0),
        Some(1),
        Some(2),
        Some(3),
        Some(4),
        Some(5),
        Some(6),
    ]);
    let field = Field::new(&quot;c1&quot;, array.data_type().clone(), true);
    write_single_array(&quot;test.parquet&quot;, &amp;array, field)
}
</code></pre></pre>
<p>For single-threaded writing, this crate offers an API that encapsulates the above logic. It 
assumes that a <code>RecordBatch</code> is mapped to a single row group with a single page per column.</p>
<pre><pre class="playground"><code class="language-rust">use std::fs::File;
use std::sync::Arc;

use arrow2::{
    array::{Array, Int32Array},
    datatypes::{Field, Schema},
    error::Result,
    io::parquet::write::{
        write_file, Compression, Encoding, RowGroupIterator, Version, WriteOptions,
    },
    record_batch::RecordBatch,
};

fn write_batch(path: &amp;str, batch: RecordBatch) -&gt; Result&lt;()&gt; {
    let schema = batch.schema().clone();

    let options = WriteOptions {
        write_statistics: true,
        compression: Compression::Uncompressed,
        version: Version::V2,
    };

    let iter = vec![Ok(batch)];

    let row_groups =
        RowGroupIterator::try_new(iter.into_iter(), &amp;schema, options, vec![Encoding::Plain])?;

    // Create a new empty file
    let mut file = File::create(path)?;

    // Write the file. Note that, at present, any error results in a corrupted file.
    let parquet_schema = row_groups.parquet_schema().clone();
    let _ = write_file(
        &amp;mut file,
        row_groups,
        &amp;schema,
        parquet_schema,
        options,
        None,
    )?;
    Ok(())
}

fn main() -&gt; Result&lt;()&gt; {
    let array = Int32Array::from(&amp;[
        Some(0),
        Some(1),
        Some(2),
        Some(3),
        Some(4),
        Some(5),
        Some(6),
    ]);
    let field = Field::new(&quot;c1&quot;, array.data_type().clone(), true);
    let schema = Schema::new(vec![field]);
    let batch = RecordBatch::try_new(Arc::new(schema), vec![Arc::new(array)])?;

    write_batch(&quot;test.parquet&quot;, batch)
}
</code></pre></pre>
<h2 id="multi-threaded-writing"><a class="header" href="#multi-threaded-writing">Multi-threaded writing</a></h2>
<p>As user of this crate, you will need to decide how you would like to parallelize,
and whether order is important. Below you can find an example where we
use <a href="https://crates.io/crates/rayon"><code>rayon</code></a> to perform the heavy lift of
encoding and compression.
This operation is <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a>
and results in a speed up equal to minimum between the number of cores
and number of columns in the record.</p>
<pre><pre class="playground"><code class="language-rust">/// Example demonstrating how to write to parquet in parallel.
use std::sync::Arc;

use rayon::prelude::*;

use arrow2::{
    array::*, datatypes::PhysicalType, error::Result, io::parquet::write::*,
    record_batch::RecordBatch,
};

fn parallel_write(path: &amp;str, batch: &amp;RecordBatch) -&gt; Result&lt;()&gt; {
    let options = WriteOptions {
        write_statistics: true,
        compression: Compression::Snappy,
        version: Version::V2,
    };
    let encodings = batch.schema().fields().par_iter().map(|field| {
        match field.data_type().to_physical_type() {
            // let's be fancy and use delta-encoding for binary fields
            PhysicalType::Binary
            | PhysicalType::LargeBinary
            | PhysicalType::Utf8
            | PhysicalType::LargeUtf8 =&gt; Encoding::DeltaLengthByteArray,
            // remaining is plain
            _ =&gt; Encoding::Plain,
        }
    });

    let parquet_schema = to_parquet_schema(batch.schema())?;

    // write batch to pages; parallelized by rayon
    let columns = batch
        .columns()
        .par_iter()
        .zip(parquet_schema.columns().to_vec().into_par_iter())
        .zip(encodings)
        .map(|((array, descriptor), encoding)| {
            // create encoded and compressed pages this column
            let encoded_pages = array_to_pages(array.as_ref(), descriptor, options, encoding)?;
            encoded_pages
                .map(|page| compress(page?, vec![], options.compression).map_err(|x| x.into()))
                .collect::&lt;Result&lt;Vec&lt;_&gt;&gt;&gt;()
        })
        .collect::&lt;Result&lt;Vec&lt;Vec&lt;CompressedPage&gt;&gt;&gt;&gt;()?;

    // create the iterator over groups (one in this case)
    // (for more batches, create the iterator from them here)
    let row_groups = std::iter::once(Result::Ok(DynIter::new(columns.iter().map(|column| {
        Ok(DynStreamingIterator::new(
            fallible_streaming_iterator::convert(column.iter().map(Ok)),
        ))
    }))));

    // Create a new empty file
    let mut file = std::fs::File::create(path)?;

    // Write the file.
    let _file_size = write_file(
        &amp;mut file,
        row_groups,
        batch.schema(),
        parquet_schema,
        options,
        None,
    )?;

    Ok(())
}

fn create_batch(size: usize) -&gt; Result&lt;RecordBatch&gt; {
    let c1: Int32Array = (0..size)
        .map(|x| if x % 9 == 0 { None } else { Some(x as i32) })
        .collect();
    let c2: Utf8Array&lt;i32&gt; = (0..size)
        .map(|x| {
            if x % 8 == 0 {
                None
            } else {
                Some(x.to_string())
            }
        })
        .collect();

    RecordBatch::try_from_iter([
        (&quot;c1&quot;, Arc::new(c1) as Arc&lt;dyn Array&gt;),
        (&quot;c2&quot;, Arc::new(c2) as Arc&lt;dyn Array&gt;),
    ])
}

fn main() -&gt; Result&lt;()&gt; {
    let batch = create_batch(10_000_000)?;

    parallel_write(&quot;example.parquet&quot;, &amp;batch)
}
</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="read-arrow"><a class="header" href="#read-arrow">Read Arrow</a></h1>
<p>When compiled with feature <code>io_ipc</code>, this crate can be used to read Arrow files.</p>
<p>An Arrow file is composed by a header, a footer, and blocks of <code>RecordBatch</code>es.
Reading it generally consists of:</p>
<ol>
<li>read metadata, containing the block positions in the file</li>
<li>seek to each block and read it</li>
</ol>
<p>The example below shows how to read them into <code>RecordBatch</code>es:</p>
<pre><pre class="playground"><code class="language-rust">use std::fs::File;

use arrow2::error::Result;
use arrow2::io::ipc::read::{read_file_metadata, FileReader};
use arrow2::io::print;
use arrow2::record_batch::RecordBatch;

fn read_batches(path: &amp;str) -&gt; Result&lt;Vec&lt;RecordBatch&gt;&gt; {
    let mut file = File::open(path)?;

    // read the files' metadata. At this point, we can distribute the read whatever we like.
    let metadata = read_file_metadata(&amp;mut file)?;

    // Simplest way: use the reader, an iterator over batches.
    let reader = FileReader::new(file, metadata, None);

    reader.collect()
}

fn main() -&gt; Result&lt;()&gt; {
    use std::env;
    let args: Vec&lt;String&gt; = env::args().collect();

    let file_path = &amp;args[1];

    let batches = read_batches(file_path)?;
    print::print(&amp;batches);
    Ok(())
}
</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="read-arrow-streams"><a class="header" href="#read-arrow-streams">Read Arrow streams</a></h1>
<p>When compiled with feature <code>io_ipc</code>, this crate can be used to read Arrow streams.</p>
<p>The example below shows how to read from a stream:</p>
<pre><pre class="playground"><code class="language-rust">use std::net::TcpStream;
use std::thread;
use std::time::Duration;

use arrow2::array::{Array, Int64Array};
use arrow2::datatypes::DataType;
use arrow2::error::Result;
use arrow2::io::ipc::read;

fn main() -&gt; Result&lt;()&gt; {
    const ADDRESS: &amp;str = &quot;127.0.0.1:12989&quot;;

    let mut reader = TcpStream::connect(ADDRESS)?;
    let metadata = read::read_stream_metadata(&amp;mut reader)?;
    let mut stream = read::StreamReader::new(&amp;mut reader, metadata);

    let mut idx = 0;
    loop {
        match stream.next() {
            Some(x) =&gt; match x {
                Ok(read::StreamState::Some(b)) =&gt; {
                    idx += 1;
                    println!(&quot;batch: {:?}&quot;, idx)
                }
                Ok(read::StreamState::Waiting) =&gt; thread::sleep(Duration::from_millis(2000)),
                Err(l) =&gt; println!(&quot;{:?} ({})&quot;, l, idx),
            },
            None =&gt; break,
        };
    }

    Ok(())
}
</code></pre></pre>
<p>e.g. written by pyarrow:</p>
<pre><code class="language-python ignore">import pyarrow as pa
from time import sleep
import socket

# Set up the data exchange socket
sk = socket.socket()
sk.bind((&quot;127.0.0.1&quot;, 12989))
sk.listen()

data = [
    pa.array([1, 2, 3, 4]),
    pa.array([&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;, None]),
    pa.array([True, None, False, True]),
]

batch = pa.record_batch(data, names=[&quot;f0&quot;, &quot;f1&quot;, &quot;f2&quot;])

# Accept incoming connection and stream the data away
connection, address = sk.accept()
dummy_socket_file = connection.makefile(&quot;wb&quot;)
with pa.RecordBatchStreamWriter(dummy_socket_file, batch.schema) as writer:
    for i in range(50):
        writer.write_batch(batch)
        sleep(1)
</code></pre>
<p>via</p>
<pre><code class="language-bash ignore">python main.py &amp;
PRODUCER_PID=$!

sleep 1 # wait for metadata to be available.
cargo run

kill $PRODUCER_PID
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="write-arrow"><a class="header" href="#write-arrow">Write Arrow</a></h1>
<p>When compiled with feature <code>io_ipc</code>, this crate can be used to write Arrow files.</p>
<p>An Arrow file is composed by a header, a footer, and blocks of <code>RecordBatch</code>es.</p>
<p>The example below shows how to write <code>RecordBatch</code>es:</p>
<pre><pre class="playground"><code class="language-rust">use std::fs::File;
use std::sync::Arc;

use arrow2::array::{Int32Array, Utf8Array};
use arrow2::datatypes::{DataType, Field, Schema};
use arrow2::error::Result;
use arrow2::io::ipc::write;
use arrow2::record_batch::RecordBatch;

fn write_batches(path: &amp;str, schema: &amp;Schema, batches: &amp;[RecordBatch]) -&gt; Result&lt;()&gt; {
    let file = File::create(path)?;

    let mut writer = write::FileWriter::try_new(file, schema)?;

    for batch in batches {
        writer.write(batch)?
    }
    writer.finish()
}

fn main() -&gt; Result&lt;()&gt; {
    use std::env;
    let args: Vec&lt;String&gt; = env::args().collect();

    let file_path = &amp;args[1];

    // create a batch
    let schema = Schema::new(vec![
        Field::new(&quot;a&quot;, DataType::Int32, false),
        Field::new(&quot;b&quot;, DataType::Utf8, false),
    ]);

    let a = Int32Array::from_slice(&amp;[1, 2, 3, 4, 5]);
    let b = Utf8Array::&lt;i32&gt;::from_slice(&amp;[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;]);

    let batch = RecordBatch::try_new(Arc::new(schema.clone()), vec![Arc::new(a), Arc::new(b)])?;

    // write it
    write_batches(file_path, &amp;schema, &amp;[batch])?;
    Ok(())
}
</code></pre></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
