[package]
name = "arrow2"
version = "0.7.0"
license = "Apache-2.0"
description = "Unofficial implementation of Apache Arrow spec in safe Rust"
homepage = "https://github.com/jorgecarleitao/arrow2"
repository = "https://github.com/jorgecarleitao/arrow2"
authors = ["Jorge C. Leitao <jorgecarleitao@gmail.com>", "Apache Arrow <dev@arrow.apache.org>"]
keywords = [ "arrow", "analytics" ]
edition = "2018"
exclude = ["testing/"]

[lib]
name = "arrow2"
bench = false

[dependencies]
num-traits = "0.2"
chrono = { version = "0.4", default_features = false, features = ["std"] }
chrono-tz = { version = "0.6", optional = true }
# To efficiently cast numbers to strings
lexical-core = { version = "0.8", optional = true }
# We need to Hash values before sending them to an hasher. This
# crate provides HashMap that assumes pre-hashed values.
hash_hasher = "^2.0.3"
# For SIMD utf8 validation
simdutf8 = "0.1.3"

# for csv io
csv = { version = "^1.1", optional = true }

# for csv async io
csv-async = { version = "^1.1", optional = true }

regex = { version = "^1.3", optional = true }
streaming-iterator = { version = "0.1", optional = true }
fallible-streaming-iterator = { version = "0.1", optional = true }

serde = { version = "^1.0", features = ["rc"], optional = true }
serde_derive = { version = "^1.0", optional = true }
serde_json = { version = "^1.0", features = ["preserve_order"], optional = true }
indexmap = { version = "^1.6", optional = true }

# used to print columns in a nice columnar format
comfy-table = { version = "4.0", optional = true, default-features = false }

arrow-format = { version = "0.3.0",  optional = true, features = ["ipc"] }

hex = { version = "^0.4", optional = true }

# for IPC compression
lz4 = { version = "1.23.1", optional = true }
zstd = { version = "0.9", optional = true }

rand = { version = "0.8", optional = true }

itertools = { version = "^0.10", optional = true }

base64 = { version = "0.13.0", optional = true }

packed_simd = { version = "0.3", optional = true, package = "packed_simd_2" }

# to write to parquet as a stream
futures = { version = "0.3", optional = true }

# for faster hashing
ahash = { version = "0.7", optional = true }

# parquet support
parquet2 = { version = "0.7", optional = true, default_features = false, features = ["stream"] }

# avro
avro-rs = { version = "0.13", optional = true, default_features = false }
# compression of avro
libflate = { version = "1.1.1", optional = true }
snap = { version = "1", optional = true }

# for division/remainder optimization at runtime
strength_reduce = { version = "0.2", optional = true }

# For instruction multiversioning
multiversion = { version = "0.6.1", optional = true }

[dev-dependencies]
criterion = "0.3"
flate2 = "1"
doc-comment = "0.3"
crossbeam-channel = "0.5.1"
# used to test async readers
tokio = { version = "1", features = ["macros", "rt", "fs"] }
tokio-util = { version = "0.6", features = ["compat"] }
# used to run formal property testing
proptest = { version = "1", default_features = false, features = ["std"] }
avro-rs = { version = "0.13", features = ["snappy"] }

[package.metadata.docs.rs]
features = ["full"]
rustdoc-args = ["--cfg", "docsrs"]

[features]
default = []
full = [
    "io_csv",
    "io_csv_async",
    "io_json",
    "io_ipc",
    "io_flight",
    "io_ipc_write_async",
    "io_ipc_compression",
    "io_json_integration",
    "io_print",
    "io_parquet",
    "io_parquet_compression",
    "io_avro",
    "io_avro_compression",
    "regex",
    "merge_sort",
    "compute",
    # parses timezones used in timestamp conversions
    "chrono-tz"
]
merge_sort = ["itertools"]
io_csv = ["io_csv_read", "io_csv_write"]
io_csv_async = ["io_csv_read_async"]
io_csv_read = ["csv", "lexical-core"]
io_csv_read_async = ["csv-async", "lexical-core", "futures"]
io_csv_write = ["csv", "streaming-iterator", "lexical-core"]
io_json = ["serde", "serde_json", "indexmap"]
io_ipc = ["arrow-format"]
io_ipc_write_async = ["io_ipc", "futures"]
io_ipc_compression = ["lz4", "zstd"]
io_flight = ["io_ipc", "arrow-format/flight-data"]
io_parquet_compression = [
    "parquet2/zstd",
    "parquet2/snappy",
    "parquet2/gzip",
    "parquet2/lz4",
    "parquet2/brotli",
]
io_avro = ["avro-rs", "fallible-streaming-iterator", "serde_json"]
io_avro_compression = [
    "libflate",
    "snap",
]
# io_json: its dependencies + error handling
# serde_derive: there is some derive around
io_json_integration = ["io_json", "serde_derive", "hex"]
io_print = ["comfy-table"]
# the compute kernels. Disabling this significantly reduces compile time.
compute = ["strength_reduce", "multiversion", "lexical-core", "ahash"]
# base64 + io_ipc because arrow schemas are stored as base64-encoded ipc format.
io_parquet = ["parquet2", "io_ipc", "base64", "futures"]
benchmarks = ["rand"]
simd = ["packed_simd"]
# uses a custom allocator whose pointers are aligned along cache lines.
# Using this features makes `Buffer` and `MutableBuffer` incompatible with `Vec`.
cache_aligned = []

[package.metadata.cargo-all-features]
skip_feature_sets = [
    # full is tested independently and is not to be used with other features.
    ["full"],
    # very small scope with no API changes.
    ["ahash"],
    ["benchmarks"],
    ["merge_sort"],
    # io are additive APIs and do not interact
    ["io_csv"],
    ["io_csv_read"],
    ["io_csv_write"],
    ["io_csv_async"],
    ["io_csv_read_async"],
    ["io_avro"],
    ["io_avro_compression"],
    ["io_json"],
    ["io_flight"],
    ["io_ipc"],
    ["io_ipc_write_async"],
    ["io_parquet"],
    ["io_json_integration"],
    # this does not change the public API
    ["io_parquet_compression"],
    ["io_ipc_compression"],
    # tested in separate
    ["simd"],
]

skip_optional_dependencies = true

[[bench]]
name = "take_kernels"
harness = false

[[bench]]
name = "filter_kernels"
harness = false

[[bench]]
name = "cast_kernels"
harness = false

[[bench]]
name = "sort_kernel"
harness = false

[[bench]]
name = "length_kernel"
harness = false

[[bench]]
name = "count_zeros"
harness = false

[[bench]]
name = "from_trusted_len_iter"
harness = false

[[bench]]
name = "growable"
harness = false

[[bench]]
name = "comparison_kernels"
harness = false

[[bench]]
name = "read_parquet"
harness = false

[[bench]]
name = "write_parquet"
harness = false

[[bench]]
name = "aggregate"
harness = false

[[bench]]
name = "write_ipc"
harness = false

[[bench]]
name = "arithmetic_kernels"
harness = false

[[bench]]
name = "bitmap"
harness = false

[[bench]]
name = "concat"
harness = false

[[bench]]
name = "bitmap_ops"
harness = false

[[bench]]
name = "write_csv"
harness = false

[[bench]]
name = "hash_kernel"
harness = false

[[bench]]
name = "iter_utf8"
harness = false

[[bench]]
name = "iter_list"
harness = false

[[bench]]
name = "avro_read"
harness = false

[[bench]]
name = "bitwise"
harness = false